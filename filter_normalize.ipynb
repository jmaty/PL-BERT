{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for filtering and normalizing Wikipedia (Czech) dataset\n",
    "\n",
    "Při rozdělení na 4 soubory po ~550 tis. odstavcích každá úloha s 8 CPU běží ~16 hod. a RAM 23 GB (`chunk_size=4`, `batch_size=1000`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca5ee4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "from tpp_ttstool import TppTtstool\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbd72d-f9ba-485c-ae33-53298eb8594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CPUS = int(os.environ[\"PBS_NUM_PPN\"])\n",
    "print(f\"> Number of CPUs: {N_CPUS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf0155",
   "metadata": {},
   "source": [
    "## Papermill options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341058b9-3740-4bab-9e45-a309fefa6eb8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "inp_paragraph_file = '../BERT_cs/WIKI_C4Cleaned1k.txt'\n",
    "inp_paragraph_file = 'test_norm.txt'\n",
    "out_sentence_file = 'test_norm.out2.txt'\n",
    "# not_supported_chars_file = 'test_not_supported_chars.txt'\n",
    "ttstool_bin= \"./tts_tool/tts_tool\"\n",
    "ttstool_data = \"./tts_tool/data/frontend_normalize.json\"\n",
    "chunk_size = 4 # number of paragraphs processed by each CPU\n",
    "batch_size = 1000 # number of sentences to write in once\n",
    "# punctuation = \".,;:-?!…\" # !!! TODO: definovat\n",
    "\n",
    "supported_chars = {\n",
    "    'lower_chars': 'aábcčdďeéěfghiíjklmnňoópqrřsštťuúůvwxyýzž',\n",
    "    'upper_chars': 'AÁBCČDĎEÉĚFGHIÍJKLMNŇOÓPQRŘSŠTŤUÚŮVWXYÝZŽ',\n",
    "    'digits':  '0123456789',\n",
    "    'white_spaces': ' ',\n",
    "    'punct': '.,;:-?',\n",
    "}\n",
    "\n",
    "replacements = {\n",
    "    'character': {\n",
    "        '–': '-',\n",
    "        '—': '-',\n",
    "        '−': '-',\n",
    "        # '\"': '',\n",
    "        # \"'\": '',\n",
    "        # \"„\": '',\n",
    "        # \"“\": '',\n",
    "    },\n",
    "    # 'word': {\n",
    "    #    '%': 'procent',\n",
    "    #},\n",
    "    'paragraph': {\n",
    "        'km/h': 'kilometrů za hodinu',\n",
    "        'm/s': 'metrů za sekundu',\n",
    "        'př.n.l.': 'před naším letopočtem',\n",
    "        'př. n. l.': 'před naším letopočtem',\n",
    "        'st. př. Kr.': 'století před Kristem',\n",
    "        'st. př. kr.': 'století před Kristem',\n",
    "        ' napr.': ' např.',\n",
    "        # '<br': ' ',\n",
    "    },\n",
    "    'postprocess': {\n",
    "        ',nebo ': ', nebo ',\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dbedf7",
   "metadata": {},
   "source": [
    "## Define global variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcf1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nejprve řadíme klíče slovníku podle délky v sestupném pořadí. To zajistí, že delší fráze budou nahrazeny před kratšími,\n",
    "# čímž se předejde nesprávným nebo duplicitním náhradám.\n",
    "paragraph_replacements = dict(sorted(replacements['paragraph'].items(), key=lambda item: len(item[0]), reverse=True))\n",
    "# Extract single dict of character replacements\n",
    "character_replacements = replacements['character']\n",
    "# Extract single dict of postprocess replacements sorted by length\n",
    "postprocess_replacements = dict(sorted(replacements['postprocess'].items(), key=lambda item: len(item[0]), reverse=True))\n",
    "\n",
    "paragraph_pattern = re.compile(\"|\".join(map(re.escape, paragraph_replacements.keys())))\n",
    "character_pattern = re.compile(r'\\s+')\n",
    "postprocess_pattern = re.compile(\"|\".join(map(re.escape, postprocess_replacements.keys())))\n",
    "\n",
    "# Ošetřete speciální znaky v interpunkčních znaménkách\n",
    "# Vytvořte nový řetězec bez '-'\n",
    "punct_no_dash = supported_chars['punct'].replace('-', '')\n",
    "escaped_punctuation = re.escape(punct_no_dash)\n",
    "# Kompilace regulárních výrazů s použitím stringu interpunkčních znamének\n",
    "pattern_before_punct = re.compile(r'\\s+([{}])'.format(escaped_punctuation))\n",
    "pattern_after_punct = re.compile(r'([{}])\\s*'.format(escaped_punctuation))\n",
    "pattern_dash = re.compile(r'(\\s*)(-)(\\s*)')\n",
    "pattern_multispace = re.compile(r'\\s+')\n",
    "pattern_html = re.compile(r'<.*?>')\n",
    "\n",
    "# Setup TPP with path to tts_tool binary and data\n",
    "normalizer = TppTtstool('cz', tts_tool_bin=ttstool_bin, tts_tool_data=ttstool_data, punct=supported_chars['punct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fabc6c-5b32-4e04-93bb-e99681a53af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_dash(match):\n",
    "    before = match.group(1)\n",
    "    dash = match.group(2)\n",
    "    after = match.group(3)\n",
    "    if (before and before.strip() == '') or (after and after.strip() == ''):\n",
    "        return ' - '\n",
    "    else:\n",
    "        return dash\n",
    "\n",
    "def replace_chars(text, pattern, replacements):\n",
    "    text = pattern.sub(text, ' ')\n",
    "    return text.translate(str.maketrans(replacements))\n",
    "\n",
    "def is_extremal_sentence(sentence, min_len=None, max_len=None, remove_neighbors=False):\n",
    "    if min_len and len(sentence) < min_len:\n",
    "        return True\n",
    "    if max_len and len(sentence) > max_len:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def check_chars(text, supported_dict):\n",
    "    # Check supported chars\n",
    "    return all(char in set(''.join(supported_dict.values())) for char in text)\n",
    "\n",
    "def check_word_level(sentence):\n",
    "    for idx, word in enumerate(sentence.split()):\n",
    "        if idx == 0 and len(word) == 1 and word.isupper():\n",
    "            # Sentence-leading preposition is OK\n",
    "            continue\n",
    "        # Check word contains uppercase chars\n",
    "        if not check_case_word(word):\n",
    "            # Acronym is not OK\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def check_case_word(word):\n",
    "    n_upper = sum(1 for c in word if c.isupper())\n",
    "    # n_lower = sum(1 for c in text if c.lower())\n",
    "    return not(n_upper > 1)\n",
    "\n",
    "def replace_supchars(text, pattern, replacements):\n",
    "    # pattern = re.compile(\"|\".join(map(re.escape, replacements.keys())))\n",
    "    return pattern.sub(lambda match: replacements[match.group(0)], text)\n",
    "\n",
    "def check_final_punct(text, punctuation):\n",
    "    if text[-1] in punctuation:\n",
    "        if text[-1] in ',;:-':\n",
    "            text = text[:-1] + '.'\n",
    "    else:\n",
    "        text += '.'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f474f-48b6-411d-af49-f851d6cc402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkce pro zpracování jedné věty\n",
    "def process_line(text):\n",
    "    global paragraph_replacements, character_replacements, supported_chars\n",
    "    global paragraph_pattern, character_pattern, postprocess_replacements, postprocess_pattern\n",
    "    global pattern_before_punct, pattern_after_punct, pattern_dash, replace_dash, pattern_multispace\n",
    "    \n",
    "    # print(\"INPUT:   \", text)\n",
    "    # Replace supcharacter texts at the paragraph level\n",
    "    paragraph = replace_supchars(text.strip(), paragraph_pattern, paragraph_replacements)\n",
    "    # print(\"PAR REPL:\", paragraph)\n",
    "    \n",
    "    # Replace unsupported characters at the paragraph level\n",
    "    paragraph = replace_chars(paragraph, character_pattern, character_replacements)\n",
    "    # print(f\"CH REPL: {paragraph}\\n\")\n",
    "\n",
    "    # Replace HTML tags\n",
    "    paragraph = pattern_html.sub(' ', paragraph)\n",
    "    \n",
    "    try:    # Parse paragraph\n",
    "        normalizer.ssml_parse(paragraph)\n",
    "    except RuntimeError as e:\n",
    "        print('[!] Parsing paragraph failed in tts_tool => skipping\\n')\n",
    "        print(paragraph)\n",
    "        print()\n",
    "        # print(e)\n",
    "        # raise\n",
    "        return [], 0   # return empty list => continue in processing next paragraphs\n",
    "    \n",
    "    # Normalize & parse to sentences\n",
    "    sentences = list(normalizer.to_sentences_orto())\n",
    "\n",
    "    # Go through individual sentences\n",
    "    correct_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # # check sentence starting with lower case\n",
    "        # if sentence[0].islower():\n",
    "        #    # remove previous sentence (probably problematic)\n",
    "        #    prev_sentence = correct_sentences.pop() if correct_sentences else ''\n",
    "        #    # skip sentence\n",
    "        #    print('[!] Sentence starts with lowercase => skipping and removing previous sentence\\n')\n",
    "        #    print(sentence)\n",
    "        #    print(prev_sentence)\n",
    "        #    print()\n",
    "        #    continue\n",
    "        \n",
    "        # Ošetření pomlčky\n",
    "        sentence = pattern_dash.sub(replace_dash, sentence)\n",
    "        if not check_chars(sentence, supported_chars):\n",
    "            continue\n",
    "        if is_extremal_sentence(sentence, 3):\n",
    "            continue\n",
    "        if not check_word_level(sentence):\n",
    "            continue\n",
    "\n",
    "        # --- Postprocess sentence\n",
    "        # sentence = check_final_punct(sentence, supported_chars['punct'])\n",
    "        # if sentence[0].islower():\n",
    "        #     sentence = sentence[0].upper() + sentence[1:]\n",
    "        # Odstraňte mezery před interpunkčními znaménky\n",
    "        sentence = pattern_before_punct.sub(r'\\1', sentence)\n",
    "        # Nahraďte více mezer po interpunkci jednou mezerou\n",
    "        sentence = pattern_after_punct.sub(r'\\1 ', sentence)\n",
    "        sentence = replace_supchars(sentence, postprocess_pattern, postprocess_replacements)\n",
    "        sentence = pattern_multispace.sub(' ', sentence)\n",
    "        # Collect the final sentence\n",
    "        correct_sentences.append(sentence.strip())\n",
    "\n",
    "    return correct_sentences, len(sentences)\n",
    "\n",
    "# Funkce pro zpracování jednoho chunku\n",
    "def process_chunk(chunk):\n",
    "    # start_time = time.perf_counter()\n",
    "    processed_sents  = []\n",
    "    n_orig_sents_in_chunk = 0\n",
    "    \n",
    "    for item in chunk['text']:\n",
    "        sents, n_orig_sents = process_line(item)\n",
    "        processed_sents.extend(sents)\n",
    "        n_orig_sents_in_chunk += n_orig_sents\n",
    "    \n",
    "    # end_time = time.perf_counter()  # Konec měření času\n",
    "    # print(f\"Čas zpracování chunku: {end_time-start_time:.4f} sekund\\n\")\n",
    "    return processed_sents, n_orig_sents_in_chunk\n",
    "\n",
    "# Funkce pro vytvoření bloků (chunků) z datasetu\n",
    "def chunks(dataset, chunk_size):\n",
    "    for i in range(0, len(dataset), chunk_size):\n",
    "        yield dataset[i:i + chunk_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720741a0-ee0e-45fd-aec0-f20f60cc4fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Načtení datasetu\n",
    "dataset = load_dataset('text', data_files=inp_paragraph_file)['train']\n",
    "\n",
    "# Init sentence counters\n",
    "n_processed_sentences, n_orig_sentences = 0, 0\n",
    "\n",
    "# Otevřeme výstupní soubor\n",
    "with open(out_sentence_file, 'a', encoding='utf-8') as f_out:\n",
    "    with ProcessPoolExecutor(max_workers=N_CPUS) as executor:\n",
    "        # Pro každý chunk z datasetu paralelně zpracováváme věty\n",
    "        futures = [executor.submit(process_chunk, chunk) for chunk in chunks(dataset, chunk_size)]\n",
    "\n",
    "        # Collected results\n",
    "        batch = []\n",
    "        \n",
    "        # Sběr výsledků a průběžné zapisování do souboru\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                results, n_sentences = future.result()\n",
    "                n_processed_sentences += len(results)\n",
    "                n_orig_sentences += n_sentences\n",
    "                batch.extend(results)  # Přidání výsledků do dávky\n",
    "\n",
    "                if len(batch) >= batch_size:\n",
    "                    f_out.write('\\n'.join(batch) + '\\n') # Write batch to file\n",
    "                    batch.clear() # Vymazání dávky pro nové výsledky\n",
    "\n",
    "            except Exception as e:\n",
    "                # raise e\n",
    "                print(e)\n",
    "                # traceback.print_exc()\n",
    "                # raise\n",
    "                continue\n",
    "\n",
    "        # Zápis zbývajících výsledků v dávce, pokud nějaké zůstaly\n",
    "        if batch:\n",
    "            f_out.write('\\n'.join(batch) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eaa13c-7050-4864-bb19-126ede33989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original sentences:  {n_orig_sentences}')\n",
    "print(f'Processed sentences: {n_processed_sentences}')\n",
    "print(f'Used %:              {(n_processed_sentences/n_orig_sentences):.2%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
