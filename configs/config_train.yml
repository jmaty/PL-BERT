log_dir: "models/cs-wikipedia"
mixed_precision: "fp16"
data_folder: "datasets/cz-wikipedia.processed"
batch_size: 24
save_interval: 50000
log_interval: 10
num_process: 1 # number of GPUs
num_steps: 1000000

dataset_params:
    tokenizer: "fav-kky/FERNET-C5"
    token_separator: " " # token used for phoneme separator (space)
    token_mask: "M" # token used for phoneme mask (M)
    word_separator: 18065 # token used for word separator (|)
    # word_separator: 291686 # token used for word separator (Êƒajmijef) - word-based phonetic CS tokenizer
    # word_separator: 3039 # token used for word separator (<formula>) - original word-based EN tokenizer
    token_maps: "token_maps.pkl"    # token map path
    symbol_dict_path: "symbol_dict.csv"  # symbol definition dictionary
    
    max_mel_length: 512 # max phoneme length
    
    word_mask_prob: 0.15 # probability to mask the entire word
    phoneme_mask_prob: 0.1 # probability to mask each phoneme
    replace_prob: 0.2 # probablity to replace phonemes
    
model_params:
    vocab_size: 81 # number of phonemes
    hidden_size: 768
    num_attention_heads: 12
    intermediate_size: 2048
    max_position_embeddings: 512
    num_hidden_layers: 12
    dropout: 0.1
