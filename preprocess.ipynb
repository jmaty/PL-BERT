{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Wikipedia (Czech) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initilizing phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import sys\n",
    "from text_utils import TextCleaner, load_symbol_dict\n",
    "from datasets import load_dataset\n",
    "from tpp_ttstool import TppTtstool\n",
    "from phonemize import phonemize\n",
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError\n",
    "\n",
    "# # Set path to compatible transformers 4.33.3 library\n",
    "# sys.path.insert(0, '/storage/plzen4-ntis/home/jmatouse/.local/transformers-4.33.3/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbd72d-f9ba-485c-ae33-53298eb8594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CPUS = int(os.environ[\"PBS_NUM_PPN\"])\n",
    "print(f\"> Number of CPUs: {N_CPUS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341058b9-3740-4bab-9e45-a309fefa6eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'configs/config.yml'\n",
    "LANG = 'cs'\n",
    "DATASET = '../BERT_cs/WIKI_C4Cleaned10.sentences.norm.txt'\n",
    "SYMBOL_PATH = 'symbol_dict.csv'\n",
    "ROOT_DIR = \"./wiki_phoneme\" # set up root directory for multiprocessor processing\n",
    "NUM_SHARDS = 1\n",
    "MAX_WORKERS = N_CPUS # change this to the number of CPU cores your machine has\n",
    "TTSTOOL_BIN = \"tts_tool/tts_tool\"\n",
    "TTSTOOL_DATA = \"tts_tool/data/frontend_ph-redu_pauses.json\"\n",
    "PUNCTUATION = \".,;:-?!â€¦\" # !!! TODO: definovat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9861ff-e820-4b74-a30e-656a1addb128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shard(i):\n",
    "    directory = f'{ROOT_DIR}/shard_{i}'\n",
    "    if os.path.exists(directory):\n",
    "        print(f'Shard {i} already exists!')\n",
    "        return\n",
    "    print(f'Processing shard {i} ...')\n",
    "    shard = dataset.shard(num_shards=num_shards, index=i)\n",
    "    processed_dataset = shard.map(lambda t: phonemize(t['text'], phonemizer, tokenizer), remove_columns=['text'])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    processed_dataset.save_to_disk(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a4cac-deb9-4cbd-8074-37e04770d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TPP with path to tts_tool binary and data\n",
    "phonemizer = TppTtstool('cz', tts_tool_bin=TTSTOOL_BIN, tts_tool_data=TTSTOOL_DATA, punct=PUNCTUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d54fe-25d4-41d5-be60-05a4f9f05530",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"configs/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))\n",
    "\n",
    "text_cleaner = TextCleaner(load_symbol_dict(SYMBOL_PATH), pad=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d58c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['dataset_params']['tokenizer']) # you can use any other tokenizers if you want to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('text', data_files=DATASET)['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff042b-f922-440e-b9c6-f2a33ca83fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b426b5d-12c9-4b83-bda7-970804fd98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in dataset:\n",
    "    phonemize(ex['text'], phonemizer, tokenizer, PUNCTUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7ca2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"./wiki_phoneme\" # set up root directory for multiprocessor processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f9dcf",
   "metadata": {},
   "source": [
    "#### Note: You will need to run the following cell multiple times to process all shards because some will fail. Depending on how fast you process each shard, you will need to change the timeout to a longer value to make more shards processed before being killed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04261364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with ProcessPool(max_workers=MAX_WORKERS) as pool:\n",
    "    pool.map(process_shard, range(NUM_SHARDS), timeout=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78caee6",
   "metadata": {},
   "source": [
    "### Collect all shards to form the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [d for d in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, d))]\n",
    "datasets = []\n",
    "for o in output:\n",
    "    directory = f'{ROOT_DIR}/{o}'\n",
    "    try:\n",
    "        shard = load_from_disk(directory)\n",
    "        datasets.append(shard)\n",
    "        print(f'{o} loaded')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1547f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset.save_to_disk(config['data_folder'])\n",
    "print('Dataset saved to %s' % config['data_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset size\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6641414-d4ed-43ca-8bf6-54180173f809",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6f6f6",
   "metadata": {},
   "source": [
    "### Remove unneccessary tokens from the pre-trained tokenizer\n",
    "The pre-trained tokenizer contains a lot of tokens that are not used in our dataset, so we need to remove these tokens. We also want to predict the word in lower cases because cases do not matter that much for TTS. Pruning the tokenizer is much faster than training a new tokenizer from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "\n",
    "file_data = FilePathDataset(dataset)\n",
    "# loader = build_dataloader(file_data, num_workers=32, batch_size=128)\n",
    "loader = build_dataloader(file_data, num_workers=1, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique tokens in the entire dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "unique_index = [special_token]\n",
    "for _, batch in enumerate(tqdm(loader)):\n",
    "    unique_index.extend(batch)\n",
    "    unique_index = list(set(unique_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each token's lower case\n",
    "\n",
    "lower_tokens = []\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    if word.lower() != word:\n",
    "        t = tokenizer.encode([word.lower()])[0]\n",
    "        lower_tokens.append(t)\n",
    "    else:\n",
    "        lower_tokens.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = (list(set(lower_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo the mapping for lower number of tokens\n",
    "\n",
    "token_maps = {}\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    word = word.lower()\n",
    "    new_t = tokenizer.encode([word.lower()])[0]\n",
    "    token_maps[t] = {'word': word, 'token': lower_tokens.index(new_t)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af11263-56ce-4953-ae83-6cc4270fc392",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd52c51-8f5e-48d6-902a-3bfa64021a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "token_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c94be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(config['dataset_params']['token_maps'], 'wb') as handle:\n",
    "    pickle.dump(token_maps, handle)\n",
    "print('Token mapper saved to %s' % config['dataset_params']['token_maps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "train_loader = build_dataloader(dataset, batch_size=4, num_workers=0, dataset_config=config['dataset_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70874215",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
