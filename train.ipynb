{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f65b772",
   "metadata": {},
   "source": [
    "## Imports & preparatory steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import os.path as osp\n",
    "import yaml\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# !!! DEBUG !!! Nastavení seedu\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "# Pokud používáte GPU:\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # Pro více GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# from accelerate.utils import LoggerType\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import AlbertConfig, AlbertModel\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "from model import MultiTaskModel\n",
    "from dataloader import build_dataloader\n",
    "from utils import length_to_mask  # , scan_checkpoint\n",
    "\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch import __version__ as torch_version\n",
    "from platform import python_version\n",
    "\n",
    "# Check CUDA is available\n",
    "assert torch.cuda.is_available(), \"CPU training is not allowed.\"\n",
    "\n",
    "# Check the number of CPUs\n",
    "N_CPUS = int(os.environ[\"PBS_NUM_PPN\"])\n",
    "\n",
    "# Limit CPU operation in pytorch to `N_CPUS`\n",
    "torch.set_num_threads(N_CPUS)\n",
    "# torch.set_num_interop_threads(N_CPUS) !!! DEBUG !!!\n",
    "\n",
    "# Set username\n",
    "USER = os.environ[\"USER\"]\n",
    "\n",
    "# GPU\n",
    "N_GPUS = torch.cuda.device_count()\n",
    "# nvidia_smi.nvmlInit()\n",
    "\n",
    "print(\" > Computational resources...\")\n",
    "print(f\" | > Number of CPUs: {N_CPUS}\")\n",
    "print(f\" | > Number of GPUs: {N_GPUS}\")\n",
    "# for idx in range(n_gpus):\n",
    "#    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(idx)\n",
    "#    print(f\" | > Device {idx}: {nvidia_smi.nvmlDeviceGetName(handle)}\")\n",
    "print(\" > Python & module versions...\")\n",
    "print(f\" | > Python:    {python_version()}\")\n",
    "print(f\" | > PyTorch:   {torch_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42258a0",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c14482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check interactive mode\n",
    "INTERACTIVE_MODE = bool(\"JupyterLab\" in os.environ[\"PBS_JOBNAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ffb10",
   "metadata": {},
   "source": [
    "### Papermill settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfbe9a0a",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "log_dir = \"models/test\"\n",
    "mixed_precision = \"fp16\"\n",
    "data_folder = \"datasets/cz-wikipedia.processed\"\n",
    "batch_size = 4\n",
    "save_interval = 50000\n",
    "log_interval = 1\n",
    "num_process = 1  # number of GPUs\n",
    "# num_steps = 1000000\n",
    "num_steps = 12\n",
    "\n",
    "dataset_params = {\n",
    "    \"tokenizer\": \"fav-kky/FERNET-C5\",\n",
    "    \"token_separator\": \" \",  # token used for phoneme separator (space)\n",
    "    \"token_mask\": \"M\",  # token used for phoneme mask (M)\n",
    "    \"word_separator\": 18065,  # token used for word separator (|)\n",
    "    \"token_maps\": \"token_maps.pkl\",  # token map path\n",
    "    \"symbol_dict_path\": \"symbol_dict.csv\",  # symbol definition dictionary\n",
    "    \"max_mel_length\": 512,  # max phoneme length\n",
    "    \"word_mask_prob\": 0.15,  # probability to mask the entire word\n",
    "    \"phoneme_mask_prob\": 0.1,  # probability to mask each phoneme\n",
    "    \"replace_prob\": 0.2,  # probablity to replace phonemes\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    \"vocab_size\": 81,  # 178\n",
    "    \"hidden_size\": 768,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"intermediate_size\": 2048,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d10cd83",
   "metadata": {},
   "source": [
    "## Copy data to scratch dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb310f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_dir = os.environ[\"SCRATCHDIR\"]\n",
    "if not INTERACTIVE_MODE:\n",
    "    # Copy dataset\n",
    "    # Prepare dataset dir in the scratch\n",
    "    print(f\"> Copying data to local scratch: {scratch_dir}\")\n",
    "    local_data_folder = os.path.join(scratch_dir, os.path.basename(data_folder))\n",
    "    shutil.copytree(data_folder, local_data_folder)\n",
    "    # local_tokenizer_folder = os.path.join(scratch_dir, os.path.basename(dataset_params[\"tokenizer\"]))\n",
    "    # shutil.copytree(dataset_params[\"tokenizer\"], local_tokenizer_folder)\n",
    "    # Store the scratch dataset so that it is used for training\n",
    "    data_folder = local_data_folder\n",
    "    # dataset_params[\"tokenizer\"] = local_tokenizer_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a64bce",
   "metadata": {},
   "source": [
    "## Create/update config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b69f32af",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"log_dir\": log_dir,\n",
    "    \"mixed_precision\": mixed_precision,\n",
    "    \"data_folder\": data_folder,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"save_interval\": save_interval,\n",
    "    \"log_interval\": log_interval,\n",
    "    \"num_process\": num_process,  # number of GPUs\n",
    "    \"num_steps\": num_steps,\n",
    "    \"dataset_params\": dataset_params,\n",
    "    \"model_params\": model_params,\n",
    "}\n",
    "\n",
    "config_file = os.path.join(scratch_dir, \"config.yml\")\n",
    "# Write to a YAML file\n",
    "with open(config_file, \"w\") as file:\n",
    "    yaml.dump(config, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d49477",
   "metadata": {},
   "source": [
    "## Run training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d2b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"dataset_params\"][\"tokenizer\"])\n",
    "logging.info(\"Vocab size: %d\", tokenizer.vocab_size)\n",
    "\n",
    "dataset = load_from_disk(config[\"data_folder\"])\n",
    "logging.info(dataset)\n",
    "\n",
    "# Load token maps\n",
    "with open(config[\"dataset_params\"][\"token_maps\"], \"rb\") as handle:\n",
    "    token_maps = pickle.load(handle)\n",
    "logging.info(\"Token maps size: %d\", len(token_maps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88691ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_loss = float(\"inf\")  # best test loss\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "num_steps = config[\"num_steps\"]\n",
    "log_interval = config[\"log_interval\"]\n",
    "save_interval = config[\"save_interval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4fa9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global dataset, config, N_CPUS\n",
    "\n",
    "    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "\n",
    "    curr_steps = 0\n",
    "    log_dir = config[\"log_dir\"]\n",
    "\n",
    "    if not osp.exists(log_dir):\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "    shutil.copy(config_file, osp.join(log_dir, osp.basename(config_file)))\n",
    "\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    train_loader = build_dataloader(\n",
    "        dataset,\n",
    "        validation=True,  # !!! DEBUG !!!\n",
    "        batch_size=batch_size,\n",
    "        num_workers=N_CPUS,\n",
    "        # num_workers=1,\n",
    "        dataset_config=config[\"dataset_params\"],\n",
    "    )\n",
    "\n",
    "    albert_base_configuration = AlbertConfig(**config[\"model_params\"])\n",
    "    bert = AlbertModel(albert_base_configuration)\n",
    "\n",
    "    num_vocab = 1 + max([m[\"token\"] for m in token_maps.values()])\n",
    "    print(f\"Num. vocab: {num_vocab}\")\n",
    "\n",
    "    bert = MultiTaskModel(\n",
    "        bert,\n",
    "        num_vocab=num_vocab,\n",
    "        # num_vocab=tokenizer.vocab_size,\n",
    "        num_tokens=config[\"model_params\"][\"vocab_size\"],\n",
    "        hidden_size=config[\"model_params\"][\"hidden_size\"],\n",
    "    )\n",
    "\n",
    "    load = True\n",
    "    try:\n",
    "        ckpts = []\n",
    "        for f in os.listdir(log_dir):\n",
    "            if f.startswith(\"step_\"):\n",
    "                ckpts.append(f)\n",
    "\n",
    "        iters = [\n",
    "            int(f.split(\"_\")[-1].split(\".\")[0])\n",
    "            for f in ckpts\n",
    "            if os.path.isfile(os.path.join(log_dir, f))\n",
    "        ]\n",
    "        iters = sorted(iters)[-1]\n",
    "    except:\n",
    "        iters = 0\n",
    "        load = False\n",
    "\n",
    "    optimizer = AdamW(bert.parameters(), lr=1e-4)\n",
    "    accelerator = Accelerator(\n",
    "        mixed_precision=config[\"mixed_precision\"], split_batches=True, kwargs_handlers=[ddp_kwargs]\n",
    "    )\n",
    "\n",
    "    if load:\n",
    "        checkpoint = torch.load(log_dir + \"/step_\" + str(iters) + \".t7\", map_location=\"cpu\")\n",
    "        state_dict = checkpoint[\"net\"]\n",
    "        from collections import OrderedDict\n",
    "\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:]  # remove `module.`\n",
    "            new_state_dict[name] = v\n",
    "\n",
    "        bert.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "        accelerator.print(\"Checkpoint loaded.\")\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    bert, optimizer, train_loader = accelerator.prepare(bert, optimizer, train_loader)\n",
    "\n",
    "    accelerator.print(\"Start training...\")\n",
    "\n",
    "    # Training is stopped after the defined number of steps\n",
    "    # => just set up a high upper bound in the range\n",
    "    for epoch in range(1, 1000000):\n",
    "        running_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            curr_steps += 1\n",
    "\n",
    "            word_ids, gt_ph_ids, masked_ph_ids, input_lengths, masked_indices = batch\n",
    "            # word_ids: list of word IDs;  each word = word IDs repeated word_length_in_phonemes times\n",
    "            # gt_ph_ids: list of ground-truth phoneme label IDs incl. punctuation and spaces between words\n",
    "            # masked_ph_ids: list of (masked) phoneme IDs incl. punctuation and spaces between words\n",
    "            # input_lengths: list of lengths of input phoneme strings\n",
    "            # masked_index: list of masked phoneme indices\n",
    "\n",
    "            text_mask = length_to_mask(torch.Tensor(input_lengths)).to(\"cuda\")\n",
    "            tokens_pred, words_pred = bert(masked_ph_ids, attention_mask=(~text_mask).int())\n",
    "\n",
    "            # P2G loss\n",
    "            loss_vocab = 0\n",
    "            for _s2s_pred, _text_input, _text_length, _masked_indices in zip(\n",
    "                words_pred, word_ids, input_lengths, masked_indices\n",
    "            ):\n",
    "                # loss_vocab += criterion(_s2s_pred[:_text_length], _text_input[:_text_length])\n",
    "                curr_loss = criterion(_s2s_pred[:_text_length], _text_input[:_text_length])\n",
    "                loss_vocab += curr_loss\n",
    "\n",
    "                # print(f\"words_ids:\\n{_text_input}\")\n",
    "                # print(f\"words_pred:\\n{_s2s_pred}\")\n",
    "                # print(f\"tokens_pred:\\n{tokens_pred}\")\n",
    "                # print(f\"loss_vocab: {curr_loss}\")\n",
    "\n",
    "            loss_vocab /= word_ids.size(0)\n",
    "            # print(f\"words size: {word_ids.size(0)}\")\n",
    "            # print(f\"avg loss_vocab: {loss_vocab}\")\n",
    "\n",
    "            # Phoneme loss\n",
    "            loss_token = 0\n",
    "            sizes = 1\n",
    "            for _s2s_pred, _text_input, _text_length, _masked_indices in zip(\n",
    "                tokens_pred, gt_ph_ids, input_lengths, masked_indices\n",
    "            ):\n",
    "                if len(_masked_indices) > 0:\n",
    "                    _text_input = _text_input[:_text_length][_masked_indices]\n",
    "                    loss_tmp = criterion(\n",
    "                        _s2s_pred[:_text_length][_masked_indices], _text_input[:_text_length]\n",
    "                    )\n",
    "                    loss_token += loss_tmp\n",
    "                    sizes += 1\n",
    "            loss_token /= sizes\n",
    "\n",
    "            # Total loss\n",
    "            loss = loss_vocab + loss_token\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            iters += 1\n",
    "            if (iters + 1) % log_interval == 0:\n",
    "                accelerator.print(\n",
    "                    \"Epoch %2d: Step [%d/%d], Loss: %.5f, Vocab Loss: %.5f, Token Loss: %.5f\"\n",
    "                    % (\n",
    "                        epoch,\n",
    "                        iters + 1,\n",
    "                        num_steps,\n",
    "                        running_loss / log_interval,\n",
    "                        loss_vocab,\n",
    "                        loss_token,\n",
    "                    )\n",
    "                )\n",
    "                running_loss = 0\n",
    "\n",
    "            if (iters + 1) % save_interval == 0:\n",
    "                accelerator.print(\"Saving...\")\n",
    "\n",
    "                state = {\n",
    "                    \"net\": bert.state_dict(),\n",
    "                    \"step\": iters,\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                }\n",
    "\n",
    "                accelerator.save(state, log_dir + \"/step_\" + str(iters + 1) + \".t7\")\n",
    "\n",
    "            if curr_steps > num_steps:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ecc55-b5c3-4b6a-912f-a9181f6057d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b697d-9ac0-4d51-9d84-4e466eb2aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train()\n",
    "# from accelerate import notebook_launcher\n",
    "# while True:\n",
    "#    notebook_launcher(train, args=(), num_processes=N_GPUS, use_port=33389)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb63f0",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e1cdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INTERACTIVE_MODE:\n",
    "    # Delete all files and subdirectories in the directory\n",
    "    for filename in os.listdir(scratch_dir):\n",
    "        file_path = os.path.join(scratch_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # remove file or symlink\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # remove directory\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}.\")\n",
    "            print(f\"Reason: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
