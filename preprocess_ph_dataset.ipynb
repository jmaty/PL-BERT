{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Wikipedia (Czech) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b68d7-73db-4547-ac4c-ee28729a9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import yaml\n",
    "import phonemizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4023ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CPUS = int(os.environ[\"PBS_NUM_PPN\"])\n",
    "print(f\"> Number of CPUs: {N_CPUS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f228b51-b28c-4471-a0b6-c32c579871d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_utils import TextCleaner\n",
    "text_cleaner = TextCleaner()\n",
    "print(f'Symbols: {len(text_cleaner)}\\n{text_cleaner.symbols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae3c55-448e-4c1c-987f-fca4d994b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'configs/config.yml'\n",
    "LANG = 'cs'\n",
    "DATASET = '../BERT_cs_phn_ipa/vety.phn.ipa.txt'\n",
    "ROOT_DIR = \"./wiki_phoneme\" # set up root directory for multiprocessor processing\n",
    "NUM_SHARDS = 100\n",
    "MAX_WORKERS = N_CPUS # change this to the number of CPU cores your machine has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc2689-7833-4773-aee5-7f609be140f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:  phonetic sentence from a phonetically transcribed dataset\n",
    "# Output: list of phonetic words IDs\n",
    "#         list of phonetic words\n",
    "def process_ph_dataset(phone_sent, tokenizer):\n",
    "    ph_words = tokenizer.tokenize(phone_sent)\n",
    "    inp_ids = [tokenizer.encode(w)[0] for w in ph_words]\n",
    "    assert len(inp_ids) == len(ph_words)\n",
    "    return {'input_ids': inp_ids, 'phonemes': ph_words}\n",
    "\n",
    "# Process shard: add phonetic word IDs and phonetic words to the dataset\n",
    "def process_shard(i):\n",
    "    directory = f'{ROOT_DIR}/shard_{i}'\n",
    "    if osp.exists(directory):\n",
    "        print(\"Shard %d already exists!\" % i)\n",
    "        return\n",
    "    print('Processing shard %d ...' % i)\n",
    "    shard = dataset.shard(num_shards=NUM_SHARDS, index=i)\n",
    "    processed_dataset = shard.map(lambda t: process_ph_dataset(t['text'], tokenizer), remove_columns=['text'])\n",
    "    if not osp.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    processed_dataset.save_to_disk(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initilizing phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587dcf89-1e41-4cab-b077-19e5b1dc0c8d",
   "metadata": {},
   "source": [
    "config = yaml.safe_load(open(CONFIG_PATH))\n",
    "\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "    language=LANG,\n",
    "    preserve_punctuation=True, \n",
    "    with_stress=False,\n",
    "    language_switch='remove-flags',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e966eb-01e6-4e8e-8c9b-5fd15fb8c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(CONFIG_PATH))\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['dataset_params']['tokenizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('text', data_files=DATASET)['train']\n",
    "# dataset = load_dataset(\"wikipedia\", \"20220301.en\")['train'] # you can use other version of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabf550-235e-4f9e-b2cd-0565b68c69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.map(lambda t: phonemize(t['text'], global_phonemizer, tokenizer), remove_columns=['text'])\n",
    "# dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4426d1-714c-4c3e-a2fb-ec2089df38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f9dcf",
   "metadata": {},
   "source": [
    "#### Note: You will need to run the following cell multiple times to process all shards because some will fail. Depending on how fast you process each shard, you will need to change the timeout to a longer value to make more shards processed before being killed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04261364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with ProcessPool(max_workers=MAX_WORKERS) as pool:\n",
    "    pool.map(process_shard, range(NUM_SHARDS), timeout=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78caee6",
   "metadata": {},
   "source": [
    "### Collect all shards to form the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568da38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = [d for d in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, d))]\n",
    "datasets = []\n",
    "for o in output:\n",
    "    directory = f'{ROOT_DIR}/{o}'\n",
    "    try:\n",
    "        shard = load_from_disk(directory)\n",
    "        datasets.append(shard)\n",
    "        print(f'{o} loaded')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1547f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset.save_to_disk(config['data_folder'])\n",
    "print(f'Dataset saved to {config[\"data_folder\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataset size\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "train_loader = build_dataloader(dataset, batch_size=4, num_workers=0, dataset_config=config['dataset_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70874215",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a923329-fa03-4629-9610-ec82464b0eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a1c92-3058-4cc6-a469-19f6e1764d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9801d3-c297-4f03-b61a-1c472d0afbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
