{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Wikipedia (Czech) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791b68d7-73db-4547-ac4c-ee28729a9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import yaml\n",
    "import phonemizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4023ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Number of CPUs: 1\n"
     ]
    }
   ],
   "source": [
    "N_CPUS = int(os.environ[\"PBS_NUM_PPN\"])\n",
    "print(f\"> Number of CPUs: {N_CPUS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f228b51-b28c-4471-a0b6-c32c579871d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbols: 178\n",
      "['$', ';', ':', ',', '.', '!', '?', '¡', '¿', '—', '…', '\"', '«', '»', '“', '”', ' ', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ɑ', 'ɐ', 'ɒ', 'æ', 'ɓ', 'ʙ', 'β', 'ɔ', 'ɕ', 'ç', 'ɗ', 'ɖ', 'ð', 'ʤ', 'ə', 'ɘ', 'ɚ', 'ɛ', 'ɜ', 'ɝ', 'ɞ', 'ɟ', 'ʄ', 'ɡ', 'ɠ', 'ɢ', 'ʛ', 'ɦ', 'ɧ', 'ħ', 'ɥ', 'ʜ', 'ɨ', 'ɪ', 'ʝ', 'ɭ', 'ɬ', 'ɫ', 'ɮ', 'ʟ', 'ɱ', 'ɯ', 'ɰ', 'ŋ', 'ɳ', 'ɲ', 'ɴ', 'ø', 'ɵ', 'ɸ', 'θ', 'œ', 'ɶ', 'ʘ', 'ɹ', 'ɺ', 'ɾ', 'ɻ', 'ʀ', 'ʁ', 'ɽ', 'ʂ', 'ʃ', 'ʈ', 'ʧ', 'ʉ', 'ʊ', 'ʋ', 'ⱱ', 'ʌ', 'ɣ', 'ɤ', 'ʍ', 'χ', 'ʎ', 'ʏ', 'ʑ', 'ʐ', 'ʒ', 'ʔ', 'ʡ', 'ʕ', 'ʢ', 'ǀ', 'ǁ', 'ǂ', 'ǃ', 'ˈ', 'ˌ', 'ː', 'ˑ', 'ʼ', '̝', '̊', 'ʰ', 'ʱ', 'ʲ', 'ʷ', 'ˠ', '˞', '↓', '↑', '→', '↗', '↘', \"'\", '̩', '̃', 'ᵻ']\n"
     ]
    }
   ],
   "source": [
    "from text_utils import TextCleaner\n",
    "text_cleaner = TextCleaner()\n",
    "print(f'Symbols: {len(text_cleaner)}\\n{text_cleaner.symbols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aae3c55-448e-4c1c-987f-fca4d994b1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'configs/config.yml'\n",
    "LANG = 'cs'\n",
    "DATASET = '../BERT_cs_phn_ipa/vety.phn.ipa.txt'\n",
    "ROOT_DIR = \"./wiki_phoneme\" # set up root directory for multiprocessor processing\n",
    "NUM_SHARDS = 100\n",
    "MAX_WORKERS = N_CPUS # change this to the number of CPU cores your machine has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29cc2689-7833-4773-aee5-7f609be140f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:  phonetic sentence from a phonetically transcribed dataset\n",
    "# Output: list of phonetic words IDs\n",
    "#         list of phonetic words\n",
    "def process_ph_dataset(phone_sent, tokenizer):\n",
    "    ph_words = tokenizer.tokenize(phone_sent)\n",
    "    inp_ids = [tokenizer.encode(w)[0] for w in ph_words]\n",
    "    assert len(inp_ids) == len(ph_words)\n",
    "    return {'input_ids': inp_ids, 'phonemes': ph_words}\n",
    "\n",
    "# Process shard: add phonetic word IDs and phonetic words to the dataset\n",
    "def process_shard(i):\n",
    "    directory = f'{ROOT_DIR}/shard_{i}'\n",
    "    if osp.exists(directory):\n",
    "        print(\"Shard %d already exists!\" % i)\n",
    "        return\n",
    "    print('Processing shard %d ...' % i)\n",
    "    shard = dataset.shard(num_shards=NUM_SHARDS, index=i)\n",
    "    processed_dataset = shard.map(lambda t: process_ph_dataset(t['text'], tokenizer), remove_columns=['text'])\n",
    "    if not osp.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    processed_dataset.save_to_disk(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initilizing phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587dcf89-1e41-4cab-b077-19e5b1dc0c8d",
   "metadata": {},
   "source": [
    "config = yaml.safe_load(open(CONFIG_PATH))\n",
    "\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "    language=LANG,\n",
    "    preserve_punctuation=True, \n",
    "    with_stress=False,\n",
    "    language_switch='remove-flags',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50e966eb-01e6-4e8e-8c9b-5fd15fb8c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open(CONFIG_PATH))\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['dataset_params']['tokenizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('text', data_files=DATASET)['train']\n",
    "# dataset = load_dataset(\"wikipedia\", \"20220301.en\")['train'] # you can use other version of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dabf550-235e-4f9e-b2cd-0565b68c69a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.map(lambda t: phonemize(t['text'], global_phonemizer, tokenizer), remove_columns=['text'])\n",
    "# dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f4426d1-714c-4c3e-a2fb-ec2089df38c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 524472\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f9dcf",
   "metadata": {},
   "source": [
    "#### Note: You will need to run the following cell multiple times to process all shards because some will fail. Depending on how fast you process each shard, you will need to change the timeout to a longer value to make more shards processed before being killed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04261364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with ProcessPool(max_workers=MAX_WORKERS) as pool:\n",
    "    pool.map(process_shard, range(NUM_SHARDS), timeout=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78caee6",
   "metadata": {},
   "source": [
    "### Collect all shards to form the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0568da38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_0 loaded\n",
      "shard_1 loaded\n",
      "shard_2 loaded\n",
      "shard_3 loaded\n",
      "shard_4 loaded\n",
      "shard_5 loaded\n",
      "shard_6 loaded\n",
      "shard_7 loaded\n",
      "shard_8 loaded\n",
      "shard_9 loaded\n",
      "shard_10 loaded\n",
      "shard_11 loaded\n",
      "shard_12 loaded\n",
      "shard_13 loaded\n",
      "shard_14 loaded\n",
      "shard_15 loaded\n",
      "shard_16 loaded\n",
      "shard_17 loaded\n",
      "shard_18 loaded\n",
      "shard_19 loaded\n",
      "shard_20 loaded\n",
      "shard_21 loaded\n",
      "shard_22 loaded\n",
      "shard_23 loaded\n",
      "shard_24 loaded\n",
      "shard_25 loaded\n",
      "shard_26 loaded\n",
      "shard_27 loaded\n",
      "shard_28 loaded\n",
      "shard_29 loaded\n",
      "shard_30 loaded\n",
      "shard_31 loaded\n",
      "shard_32 loaded\n",
      "shard_33 loaded\n",
      "shard_34 loaded\n",
      "shard_35 loaded\n",
      "shard_36 loaded\n",
      "shard_37 loaded\n",
      "shard_38 loaded\n",
      "shard_39 loaded\n",
      "shard_40 loaded\n",
      "shard_41 loaded\n",
      "shard_42 loaded\n",
      "shard_43 loaded\n",
      "shard_44 loaded\n",
      "shard_45 loaded\n",
      "shard_46 loaded\n",
      "shard_47 loaded\n",
      "shard_48 loaded\n",
      "shard_49 loaded\n",
      "shard_50 loaded\n",
      "shard_51 loaded\n",
      "shard_52 loaded\n",
      "shard_53 loaded\n",
      "shard_54 loaded\n",
      "shard_55 loaded\n",
      "shard_56 loaded\n",
      "shard_57 loaded\n",
      "shard_58 loaded\n",
      "shard_59 loaded\n",
      "shard_60 loaded\n",
      "shard_61 loaded\n",
      "shard_62 loaded\n",
      "shard_63 loaded\n",
      "shard_64 loaded\n",
      "shard_65 loaded\n",
      "shard_66 loaded\n",
      "shard_67 loaded\n",
      "shard_68 loaded\n",
      "shard_69 loaded\n",
      "shard_70 loaded\n",
      "shard_71 loaded\n",
      "shard_72 loaded\n",
      "shard_73 loaded\n",
      "shard_74 loaded\n",
      "shard_75 loaded\n",
      "shard_76 loaded\n",
      "shard_77 loaded\n",
      "shard_78 loaded\n",
      "shard_79 loaded\n",
      "shard_80 loaded\n",
      "shard_81 loaded\n",
      "shard_82 loaded\n",
      "shard_83 loaded\n",
      "shard_84 loaded\n",
      "shard_85 loaded\n",
      "shard_86 loaded\n",
      "shard_87 loaded\n",
      "shard_88 loaded\n",
      "shard_89 loaded\n",
      "shard_90 loaded\n",
      "shard_91 loaded\n",
      "shard_92 loaded\n",
      "shard_93 loaded\n",
      "shard_94 loaded\n",
      "shard_95 loaded\n",
      "shard_96 loaded\n",
      "shard_97 loaded\n",
      "shard_98 loaded\n",
      "shard_99 loaded\n"
     ]
    }
   ],
   "source": [
    "output = [d for d in os.listdir(ROOT_DIR) if os.path.isdir(os.path.join(ROOT_DIR, d))]\n",
    "datasets = []\n",
    "for o in output:\n",
    "    directory = f'{ROOT_DIR}/{o}'\n",
    "    try:\n",
    "        shard = load_from_disk(directory)\n",
    "        datasets.append(shard)\n",
    "        print(f'{o} loaded')\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1547f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d24013bb98f41dd9bc2aa7f0958dbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/524472 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to datasets/cz-phon-sentences.processed\n"
     ]
    }
   ],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset.save_to_disk(config['data_folder'])\n",
    "print(f'Dataset saved to {config[\"data_folder\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'phonemes'],\n",
       "    num_rows: 524472\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset size\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "train_loader = build_dataloader(dataset, batch_size=4, num_workers=0, dataset_config=config['dataset_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70874215",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a923329-fa03-4629-9610-ec82464b0eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1817,   1817,   1817,   1817,   1817,   1817, 291686,     15,     15,\n",
       "        291686,      8,      8, 291686,     88,     88,     88,     88, 291686,\n",
       "            77,     77,     77,     77, 291686,  58674,  58674,  58674,  58674,\n",
       "         58674,  58674,  58674,  58674, 291686,  10744,  10744,  10744,  10744,\n",
       "         10744,  10744,  10744,  10744,  10744,  10744, 291686,      7, 291686,\n",
       "        141155, 141155, 141155, 141155, 141155, 141155, 141155, 141155, 141155,\n",
       "        141155, 141155, 291686,      8,      8, 291686, 282857, 282857, 282857,\n",
       "        282857, 282857, 282857, 291686,      5, 291686,     48,     48,     48,\n",
       "            48,     48,     48, 291686,     19,     19, 291686,    724,    724,\n",
       "           724,    724,    724,    724,    724,    724,    724,    724,    724,\n",
       "           724, 291686,    382,    382,    382,    382,    382,    382,    382,\n",
       "           382, 291686,  30248,  30248,  30248,  30248,  30248,  30248,  30248,\n",
       "         30248,  30248,  30248,  30248,  30248,  30248, 291686,   2065,   2065,\n",
       "          2065,   2065,   2065,   2065,   2065,   2065, 291686,  87962,  87962,\n",
       "         87962,  87962,  87962,  87962,  87962,  87962,  87962,  87962,  87962,\n",
       "         87962,  87962, 291686,    380,    380,    380,    380,    380,    380,\n",
       "           380,    380, 291686,      5, 291686,    271,    271,    271,    271,\n",
       "        291686,     11,     11, 291686,   2253,   2253,   2253,   2253,   2253,\n",
       "          2253,   2253,   2253,   2253,   2253,   2253,   2253,   2253,   2253,\n",
       "          2253, 291686,    150,    150,    150,    150,    150,    150, 291686,\n",
       "          2856,   2856,   2856,   2856,   2856,   2856,   2856,   2856,   2856,\n",
       "          2856,   2856,   2856, 291686,     25,     25,     25,     25, 291686,\n",
       "         30248,  30248,  30248,  30248,  30248,  30248,  30248,  30248,  30248,\n",
       "         30248,  30248,  30248,  30248, 291686,    380,    380,    380,    380,\n",
       "           380,    380,    380,    380, 291686,      7, 291686,    544,    544,\n",
       "           544,    544,    544,    544,    544,    544,    544, 291686,      6,\n",
       "        291686])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "192a1c92-3058-4cc6-a469-19f6e1764d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 57,  58,  62, 131,  43,  56,  16,  44,  51,  16,  61,  47,  16,  62,\n",
       "         47,  46,  51,  16,  55, 114,  47,  54,  16,  56,  43,  58,  60,  43,\n",
       "         64,  46,  63,  16,  61,  58,  57,  54,  47,  50,  56,  57, 135,  62,\n",
       "         16,  43,  16,  56,  47,  61,  66,  57,  64,  43, 158,  64,  43,  62,\n",
       "         16,  61,  47,  16,  68,  43,  54,  57,  44,  51,  16,   3,  16,  53,\n",
       "         62,  47,  60,  43, 158,  16,  64,  47,  16,  61,  53,  63,  62,  47,\n",
       "         62, 131,  56,  57,  61,  45,  51,  16,  68,  56,  43,  55,  47,  56,\n",
       "         43, 158,  16,  58,  60,  57,  60,  63, 158,  61,  62,  43, 158, 114,\n",
       "         51, 158,  16,  55,  47, 158,  46,  51,  52,  51, 158,  16,  46,  57,\n",
       "         58,  60,  43,  53,  62,  51,  62,  61,  53,  47, 158,  16,  58,  57,\n",
       "         54,  51,  62,  51,  53,  51,  16,   3,  16,  62,  61,  57, 147,  16,\n",
       "         52,  47,  16,  58,  60, 161, 162,  51,  56,  47, 102,  55,  47,  56,\n",
       "        131,  51, 158,  55,  16,  61,  62,  47, 102, 114,  47,  16,  56,  47,\n",
       "         44,  47,  61,  58,  47,  62, 131,  56,  47, 158,  16,  52,  43,  53,\n",
       "         57,  16,  58,  60,  57,  60,  63, 158,  61,  62,  43, 158, 114,  51,\n",
       "        158,  16,  58,  57,  54,  51,  62,  51,  53,  51,  16,  43,  16,  47,\n",
       "         53,  57,  56,  57,  55,  51,  53,  51,  16,   4,  16])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f9801d3-c297-4f03-b61a-1c472d0afbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 29,  29,  29,  29,  29,  29,  16,  44,  51,  16,  61,  47,  16,  62,\n",
       "         47,  46,  51,  16,  55, 114,  47,  54,  16,  29,  29,  29,  29,  29,\n",
       "         29,  29,  29,  16,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,\n",
       "         16,  29,  16,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,\n",
       "         16,  61,  47,  16,  68,  43,  54,  57,  44,  51,  16,   3,  16,  53,\n",
       "         62,  47,  60,  43, 158,  16,  64,  47,  16,  61,  53,  63,  62,  47,\n",
       "         62, 131,  56,  57,  61,  45,  51,  16,  68,  56,  43,  55,  47,  56,\n",
       "         43, 158,  16,  58,  60,  57,  60,  63, 158,  61,  62,  43, 158, 114,\n",
       "         51, 158,  16,  55,  47, 158,  46,  51,  52,  51, 158,  16,  46,  57,\n",
       "         58,  60,  43,  53,  62,  51,  62,  61,  53,  47, 158,  16,  58,  57,\n",
       "         54,  51,  62,  51,  53,  51,  16,   3,  16,  62,  61,  57, 147,  16,\n",
       "         52,  47,  16,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,  29,\n",
       "         29,  29,  29,  29,  16,  61,  62,  47, 102, 114,  47,  16,  56,  47,\n",
       "         44,  47,  61,  58,  47,  62, 131,  56,  47, 158,  16,  52,  43,  53,\n",
       "         57,  16,  58,  60,  57,  60,  63, 158,  61,  62,  43, 158, 114,  51,\n",
       "        158,  16,  58,  57,  54,  51,  62,  51,  53,  51,  16,  43,  16,  47,\n",
       "         53,  57,  56,  57,  55,  51,  53,  51,  16,   4,  16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phonemes[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
