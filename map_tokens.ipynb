{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook token mapping for Wikipedia (Czech) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "# # Set path to compatible transformers 4.33.3 library\n",
    "# sys.path.insert(0, '/storage/plzen4-ntis/home/jmatouse/.local/transformers-4.33.3/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3435d",
   "metadata": {},
   "source": [
    "### Papermill options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939aa3a",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "log_dir = \"models/test\"\n",
    "mixed_precision = \"fp16\"\n",
    "data_folder = \"datasets/cz-wikipedia.processed\"\n",
    "batch_size = 32\n",
    "save_interval = 100\n",
    "log_interval = 10\n",
    "num_process = 1 # number of GPUs\n",
    "num_steps = 1000000\n",
    "\n",
    "dataset_params = {\n",
    "    \"tokenizer\": \"fav-kky/FERNET-C5\",\n",
    "    \"token_separator\": \" \", # token used for phoneme separator (space)\n",
    "    \"token_mask\": \"M\", # token used for phoneme mask (M)\n",
    "    \"word_separator\": 18065, # token used for word separator (|)\n",
    "    \"token_maps\": \"token_maps.pkl\", # token map path\n",
    "    \"symbol_dict_path\": \"symbol_dict.csv\",  # symbol definition dictionary\n",
    "    \n",
    "    \"max_mel_length\": 512, # max phoneme length\n",
    "    \n",
    "    \"word_mask_prob\": 0.15, # probability to mask the entire word\n",
    "    \"phoneme_mask_prob\": 0.1, # probability to mask each phoneme\n",
    "    \"replace_prob\": 0.2, # probablity to replace phonemes\n",
    "}\n",
    "    \n",
    "model_params = {\n",
    "    \"vocab_size\": 81, # 178\n",
    "    \"hidden_size\": 768,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"intermediate_size\": 2048,\n",
    "    \"max_position_embeddings\": 512,\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bbd72d-f9ba-485c-ae33-53298eb8594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CPUS = int(os.environ[\"PBS_NUM_PPN\"])\n",
    "print(f\"> Number of CPUs: {N_CPUS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Load tokenizer and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(dataset_params['tokenizer'])\n",
    "dataset = load_from_disk(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6f6f6",
   "metadata": {},
   "source": [
    "### Remove unneccessary tokens from the pre-trained tokenizer\n",
    "The pre-trained tokenizer contains a lot of tokens that are not used in our dataset, so we need to remove these tokens. We also want to predict the word in lower cases because cases do not matter that much for TTS. Pruning the tokenizer is much faster than training a new tokenizer from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = FilePathDataset(dataset)\n",
    "loader = build_dataloader(file_data, num_workers=N_CPUS, batch_size=128)\n",
    "special_token = dataset_params['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique word token IDs in the entire dataset\n",
    "# Result: list of word IDs, where word ID = tuple of 1 or more word piece IDs\n",
    "print(\"Get unique word IDs...\")\n",
    "\n",
    "unique_word_ids = [special_token]\n",
    "# for _, batch in enumerate(tqdm(loader)):\n",
    "for batch in loader:\n",
    "    unique_word_ids.extend(batch)\n",
    "    unique_word_ids = list(set(unique_word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get each token's lower case\n",
    "print(\"Get each token's lower case...\")\n",
    "\n",
    "# lower tokens are lists:\n",
    "# - either with single item\n",
    "# - or multiple items when lowercasing leads to word pieces\n",
    "lower_tokens = []\n",
    "# for t in tqdm(unique_word_ids):\n",
    "for t in unique_word_ids:\n",
    "    word = tokenizer.decode(t)\n",
    "    word_lower = word.lower()\n",
    "    if word_lower != word:\n",
    "        t = tokenizer.encode(word_lower, add_special_tokens=False)\n",
    "    lower_tokens.append(t if isinstance(t, list) else [t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the mapping for lower number of tokens\n",
    "print(\"Redo mapping for lower number of tokens...\")\n",
    "\n",
    "token_maps = {}\n",
    "# for t in tqdm(unique_word_ids):\n",
    "for t in unique_word_ids:\n",
    "    word = tokenizer.decode(t)\n",
    "    word_lower = word.lower()\n",
    "    if word_lower.startswith('##'):\n",
    "        new_t = [tokenizer.convert_tokens_to_ids(word_lower)]\n",
    "    else:\n",
    "        word_pieces = tokenizer.tokenize(word_lower, add_special_tokens=False)\n",
    "        new_t = [tokenizer.convert_tokens_to_ids(word_piece) for word_piece in word_pieces]\n",
    "    token_maps[t] = {'word': word_lower, 'token': lower_tokens.index(new_t)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c94be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_params['token_maps'], 'wb') as handle:\n",
    "    pickle.dump(token_maps, handle)\n",
    "print(f\"Token mapper saved to {dataset_params['token_maps']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "# Test dataloader\n",
    "train_loader = build_dataloader(dataset, validation=True, batch_size=4, num_workers=0, dataset_config=dataset_params)\n",
    "# Test next item in the dataloader\n",
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c83fc9",
   "metadata": {},
   "source": [
    "## Poznámky\n",
    "\n",
    "Spuštění na 16 CPU trvá cca 30 hod."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
